[
  {
    "slug": "selective-trust-head",
    "title": "Selective Trust Head — Inference-Time Reliability for LLMs",
    "blurb": "Ranks LLM answers by trustworthiness using single-forward-pass uncertainty signals (selective prediction).",
    "highlights": [
      "Leakage-safe features from token distributions (entropy, margin, prob mass).",
      "Optimized for ranking (Average Precision) in an imbalanced setting.",
      "Reported ROC-AUC, AP, Brier, and calibration behavior."
    ],
    "skills": ["LLM Reliability", "Selective Prediction", "Calibration", "Feature Engineering", "Natural Language Processing", "Abstention", "Supervised Learning", "Class-Imbalance", "Tokenization", "Sliding Windows"],
    "languages": ["Python", "LaTeX"],
    "tools": ["scikit-learn", "NumPy", "Pandas", "Hugging Face Transformers", "DistilBERT"],
    "featured": 1,
    "featuredOrder": 1,
    "domain": "ML - NLP",
    "year": 2025,
    "coverImage": "/projects/selective-trust-head.jpg",
    "links": {
      "github": "https://github.com/PetersQuinn/ReliabilityInLLMs",
      "paper": "",
      "demo": ""
    },
    "stats": [
      { "label": "Task", "value": "Selective prediction / ranking" },
      { "label": "Signals", "value": "Single forward pass" },
      { "label": "Metrics", "value": "AP, ROC-AUC, Brier, ECE" }
    ],
    "sections": [
      {
        "title": "What",
        "body": [
          "LLMs often sound confident while being wrong. This project trains a lightweight trust head that scores answers by reliability using only inference-time uncertainty signals from a single forward pass.",
          "Rather than binary correctness, the objective is selective prediction: rank answers so that the most trustworthy outputs can be prioritized under limited coverage."
        ]
      },
      {
        "title": "How",
        "body": [
          "Engineered leakage-safe uncertainty features from token-level start and end distributions, including entropy, span-vs-null margins, top-2 gaps, and probability mass summaries.",
          "Trained simple trust heads such as logistic regression to map these features to a scalar trust score, optimizing ranking quality under class imbalance using Average Precision."
        ]
      },
      {
        "title": "Results",
        "body": [
          "Achieved strong ranking performance despite a signal-limited regime in which multiple model families converged to nearly identical selective curves.",
          "Evaluated probability alignment and reliability using Brier score and Expected Calibration Error to assess when trust scores behave meaningfully as probabilities."
        ]
      },
      {
        "title": "Key Takeaways",
        "body": [
          "Inference-time uncertainty contains real but limited separable signal, making it effective for triage rather than full verification.",
          "The primary value lies in prioritization: surfacing high-trust answers first and routing low-trust outputs to abstention or verification."
        ]
      }
    ],
    "images": [
      "/projects/selective-trust-head-1.jpg",
      "/projects/selective-trust-head-2.jpg",
      "/projects/selective-trust-head.jpg"
    ]
  }
,
  {
    "slug": "fully-local-email-rag",
    "title": "Fully Local Personal Email RAG System",
    "blurb": "Privacy-first Retrieval-Augmented Generation system that lets users query personal Gmail data using a fully local LLM with no external APIs or cloud inference.",
    "highlights": [
      "End-to-end local RAG pipeline with Gmail, PDF, and DOCX ingestion.",
      "Strict per-user vector isolation with zero cross-user leakage.",
      "Sub-100ms embedding and retrieval latency in a fully offline setup."
    ],
    "skills": ["Retrieval-Augmented Generation", "Privacy-Preserving ML", "System Design", "Embedding-Based Information Retrieval", "Local LLMs", "Document Parsing"],
    "languages": ["Python", "SQL"],
    "tools": ["Streamlit", "ChromaDB", "Ollama", "sentence-transformers", "PyPDF", "python-docx", "Google OAuth"],
    "featured": 1,
    "featuredOrder": 4,
    "domain": "ML - Systems",
    "year": 2025,
    "coverImage": "/projects/local-email-rag.jpg",
    "links": {
      "github": "https://github.com/PetersQuinn/PersonalEmailRAGSystem",
      "paper": "https://github.com/PetersQuinn/PersonalEmailRAGSystem/blob/main/EvaluationReport.md",
      "demo": ""
    },
    "stats": [
      { "label": "LLM", "value": "Qwen-8B (local)" },
      { "label": "Data", "value": "Gmail + PDF/DOCX" },
      { "label": "Latency", "value": "<100ms embed + retrieve" }
    ],
    "sections": [
      {
        "title": "What",
        "body": [
          "Built a privacy-first Retrieval-Augmented Generation system that allows users to query their personal Gmail inbox using a fully local LLM.",
          "The system supports attachment-aware search, enabling PDF and DOCX documents to be retrieved, cited, and reasoned over alongside email content."
        ]
      },
      {
        "title": "How",
        "body": [
          "Ingested Gmail inboxes via OAuth and parsed email bodies plus PDF and DOCX attachments.",
          "Embedded content using a local sentence-transformer and stored vectors in per-user ChromaDB collections to enforce strict isolation.",
          "Implemented similarity retrieval, prompt assembly, and answer generation with Qwen-8B running locally via Ollama, with a Streamlit-based UI."
        ]
      },
      {
        "title": "Results",
        "body": [
          "Successfully answered inbox-only and document-specific queries with citation-grounded responses.",
          "Demonstrated zero cross-user leakage, with unauthorized queries reliably triggering refusal behavior.",
          "Achieved sub-100ms embedding and retrieval latency with fully offline, end-to-end execution."
        ]
      }
    ],
    "images": [
        "/projects/local-email-rag-1.jpg",
      "/projects/local-email-rag-2.jpg",
      "/projects/local-email-rag.jpg",
      "/projects/local-email-rag-3.jpg"
    ]
  }
,
{
  "slug": "nfl-kicker-field-goal-probability",
  "title": "NFL Kicker Field-Goal Make Probability Model (Ensemble ML)",
  "blurb": "A calibrated ensemble machine learning system that estimates NFL field-goal make probabilities using play-by-play, weather, and stadium context, designed for real decision support rather than binary prediction.",
  "highlights": [
    "Engineered contextual and environmental features including distance, wind, rain, roof state, and high-pressure situations.",
    "Trained and evaluated a diverse suite of probabilistic models with an emphasis on calibration and decision quality.",
    "Built a weighted ensemble that outperformed all single models in accuracy, calibration, and robustness."
  ],
  "skills": [
    "Probabilistic Modeling",
    "Model Calibration",
    "Ensemble Learning",
    "Sports Analytics",
    "Decision Support Systems",
    "Feature Engineering",
    "Model Evaluation"
  ],
  "languages": ["Python"],
  "tools": [
    "scikit-learn",
    "LightGBM",
    "PyBART",
    "Statsmodels",
    "Pandas",
    "NumPy",
    "Matplotlib",
    "Streamlit"
  ],
  "featured": 1,
  "featuredOrder": 3,
  "domain": "ML - Sports",
  "year": 2025,
  "coverImage": "/projects/nfl-kicker.png",
  "links": {
    "github": "https://github.com/PetersQuinn/KickersGalore-",
    "paper": "https://github.com/PetersQuinn/KickersGalore-/blob/main/NFL_Kicker_Project___Final_Report%20(1).pdf",
    "demo": "https://kickersgalore-nsakx3nxwlbgtdketifwma.streamlit.app/"
  },
  "stats": [
    { "label": "Brier Score", "value": "0.10976 (Ensemble)" },
    { "label": "AUC", "value": "0.7636" },
    { "label": "ECE@10", "value": "0.0131" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built a machine learning system to estimate the probability that an NFL field goal attempt will be successful.",
        "The objective was to produce calibrated probabilities that can plug directly into decision tools such as expected points, win probability, and fourth-down strategy models."
      ]
    },
    {
      "title": "How",
      "body": [
        "Constructed a dataset of NFL field goal attempts using historical play-by-play, stadium, and weather data spanning multiple seasons.",
        "Engineered key contextual features including kick distance, wind speed, precipitation indicators, roof state, surface type, and pressure-sensitive game situations.",
        "Trained multiple probabilistic models including Logistic Regression, Bayesian Logistic Regression, GAMs, Bagging, LightGBM, and Bayesian Additive Regression Trees (BART).",
        "Applied distance-aware calibration and built a weighted ensemble that combines model probability outputs to improve robustness in unusual or high-leverage scenarios.",
        "Designed and deployed an interactive Streamlit application that allows users to input kick conditions and receive real-time probability estimates."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Achieved a best-in-class Brier Score of 0.10976 using a weighted ensemble, outperforming all individual models.",
        "Delivered strong discrimination and reliability with AUC = 0.7636, PR-AUC(miss) = 0.3192, and ECE@10 = 0.0131.",
        "Demonstrated that ensembling reduced variance and improved stability, particularly in edge cases where single models were overconfident or poorly calibrated."
      ]
    }
  ],
  "images": [
    "/projects/nfl-kicker.png",
    "/projects/nfl-kicker-1.png",
    "/projects/nfl-kicker-2.png"
  ]
}
,
{
  "slug": "blockchain-fraud-detection",
  "title": "Blockchain Fraud Detection with Graph & Unsupervised Anomaly Models",
  "blurb": "Unsupervised fraud detection pipeline for Ethereum transfers that combines graph-based network features with anomaly-scoring models to surface suspicious activity at scale.",
  "highlights": [
    "Parsed millions of Ethereum transfers into shard-aware transaction networks.",
    "Combined graph-structural features with autoencoder reconstruction error and Isolation Forest scores.",
    "Produced ranked, threshold-adjustable anomaly outputs for adaptive fraud detection."
  ],
  "skills": [
    "Anomaly Detection",
    "Graph-Based Machine Learning",
    "Network Analysis",
    "Risk Scoring",
    "Large-Scale Data Processing",
    "Unsupervised Learning",
    "Feature Engineering"
  ],
  "languages": ["Python"],
  "tools": [
    "scikit-learn",
    "NetworkX",
    "NumPy",
    "Pandas",
    "Matplotlib",
    "PyTorch"
  ],
  "featured": 1,
  "featuredOrder": 2,
  "domain": "ML - Finance",
  "year": 2025,
  "coverImage": "/projects/blockchain-fraud.png",
  "links": {
    "github": "https://github.com/PetersQuinn/BlockChainProject",
    "paper": "",
    "demo": ""
  },
  "stats": [
    { "label": "Models", "value": "Autoencoder, Isolation Forest, KMeans" },
    { "label": "Data", "value": "1,000 Ethereum blocks" },
    { "label": "Output", "value": "Ranked anomaly scores" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built an unsupervised fraud detection system for Ethereum transactions that identifies anomalous behavior without labeled fraud data.",
        "The goal was to surface suspicious addresses and transfers by combining behavioral anomalies with graph-structural signals."
      ]
    },
    {
      "title": "How",
      "body": [
        "Parsed millions of Ethereum transfers, engineered address-level graph features, and constructed shard-aware transaction networks.",
        "Trained an autoencoder, Isolation Forest, and KMeans to model normal transaction behavior using reconstruction error and anomaly scores.",
        "Computed node-level network metrics and combined them with model outputs to produce unified risk scores."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Developed a threshold-adjustable anomaly detection pipeline that adapts sensitivity based on risk tolerance.",
        "Generated ranked anomaly outputs and demonstrated that graph features materially improved the stability and distinctiveness of fraud detection."
      ]
    }
  ],
  "images": [
    "/projects/blockchain-fraud.png",
    "/projects/blockchain-fraud-1.png",
    "/projects/blockchain-fraud-2.png"
  ]
}
,
{
  "slug": "project-performance-insights-ppi",
  "title": "Project Performance Insights (PPI)",
  "blurb": "A Streamlit-based, LLM-powered dashboard for extracting insights, detecting risks, and tracking KPIs across complex project lifecycles using document ingestion and snapshot comparison.",
  "highlights": [
    "Built an end-to-end project intelligence system that ingests documents, tracks snapshots, and surfaces executive-level insights.",
    "Used Azure OpenAI to generate summaries, detect emerging risks, and analyze changes across project updates.",
    "Designed a modular ingestion and comparison pipeline supporting structured KPIs and unstructured text."
  ],
  "skills": [
    "LLM-Powered Analytics",
    "Document Ingestion Pipelines",
    "Risk Detection",
    "KPI Tracking",
    "Snapshot Comparison",
    "Executive Reporting",
    "System Design"
  ],
  "languages": ["Python", "SQL"],
  "tools": [
    "Streamlit",
    "Azure OpenAI",
    "SQLite",
    "Pandas",
    "Python-docx",
    "openpyxl"
  ],
  "featured": 0,
  "featuredOrder": 7,
  "domain": "ML - Systems",
  "year": 2025,
  "coverImage": "/projects/ppi.png",
  "links": {
    "github": "https://github.com/PetersQuinn/executive_insights",
    "paper": "",
    "demo": ""
  },
  "stats": [
    { "label": "Interface", "value": "Streamlit Dashboard" },
    { "label": "LLM Provider", "value": "Azure OpenAI (GPT-4o mini)" },
    { "label": "Storage", "value": "SQLite (snapshot-based)" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built a project intelligence dashboard to help teams extract insights, track KPIs, and identify risks across long-running project lifecycles.",
        "The system focuses on executive-level visibility by summarizing complex project updates and highlighting meaningful changes over time."
      ]
    },
    {
      "title": "How",
      "body": [
        "Developed a multi-page Streamlit application for viewing active project insights and historical KPI trends.",
        "Implemented a modular ingestion pipeline supporting DOCX, Excel, transcripts (VTT), and early-stage parsers for emails, PDFs, and presentations.",
        "Stored structured project snapshots in a SQLite database, enabling snapshot-to-snapshot comparison without relying on raw documents.",
        "Used Azure OpenAI with project-specific prompts to generate executive summaries, detect emerging risks, and analyze cross-snapshot deltas.",
        "Built comparison and risk-detection utilities to surface changes in scope, budget, timeline, sentiment, and KPIs."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Demonstrated reliable extraction of structured project snapshots and executive summaries from heterogeneous inputs.",
        "Enabled longitudinal analysis of project performance, risks, and trends across multiple updates.",
        "Delivered a scalable, extensible architecture that can support richer parsers, caching, and integrations with collaboration tools."
      ]
    }
  ],
  "images": [
    "/projects/ppi.png",
    "/projects/ppi-1.png"
  ]
}
,
{
  "slug": "job-application-assistant",
  "title": "Job Application Assistant & Intelligent Tracker",
  "blurb": "A smart job application assistant that parses unstructured job descriptions, emails, and application responses into a centralized, searchable system with AI-driven insights.",
  "highlights": [
    "Parsed job descriptions, emails, and application responses into structured job records.",
    "Prevented duplicate entries using GPT-based matching with a manual override option.",
    "Enabled AI-assisted analysis of job fit, progress tracking, and skill gaps."
  ],
  "skills": [
    "Information Extraction",
    "Natural Language Processing",
    "Workflow Automation",
    "Decision Support Systems",
    "Data Modeling",
    "Human-in-the-Loop AI",
    "UI/UX Design"
  ],
  "languages": ["Python", "HTML"],
  "tools": [
    "Streamlit",
    "SQLite",
    "OpenAI GPT-4o mini",
    "Pandas",
    "NumPy"
  ],
  "featured": 0,
  "featuredOrder": 3,
  "domain": "ML - Systems",
  "year": 2025,
  "coverImage": "/projects/job-assistant.png",
  "links": {
    "github": "",
    "paper": "",
    "demo": ""
  },
  "stats": [
    { "label": "App Type", "value": "Multi-page Streamlit Dashboard" },
    { "label": "Storage", "value": "SQLite Database" },
    { "label": "Inputs", "value": "Job Descriptions, Emails, Application Responses" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built a smart job application assistant that centralizes and structures the entire job search process.",
        "The system helps users track applications, analyze opportunities, and reflect on their readiness for different roles."
      ]
    },
    {
      "title": "How",
      "body": [
        "Developed a multi-page Streamlit app using Python, SQLite, and GPT-4o mini.",
        "Parsed unstructured text inputs into structured job metadata including role, company, status, and matched skills.",
        "Used GPT-based similarity checks to prevent duplicate entries while allowing manual override when needed.",
        "Designed a customizable UI for job tracking, profile management, and AI-assisted evaluation."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Streamlined the job application workflow into a single intelligent assistant.",
        "Enabled dynamic tracking of application progress and clearer identification of strong-fit opportunities.",
        "Empowered users to identify skill gaps and take targeted actions to improve job readiness."
      ]
    }
  ],
  "images": [
    "/projects/job-assistant.png",
    "/projects/job-assistant-1.png",
    "/projects/job-assistant-2.png"
  ]
}

,{
  "slug": "cfp-ranking-bias-simulation",
  "title": "Systemic Bias in College Football Playoff Rankings",
  "blurb": "A simulation-based study of how preseason rankings and perception-driven biases propagate through College Football Playoff rankings, affecting accuracy, volatility, and playoff access.",
  "highlights": [
    "Simulated inverse preseason rankings to quantify how perception bias propagates through a season.",
    "Designed and compared standard vs outcome-driven (harsher) committee scoring systems.",
    "Evaluated tradeoffs between ranking accuracy, stability, and fan-facing volatility."
  ],
  "skills": [
    "Simulation Modeling",
    "Bias Analysis",
    "Ranking Systems",
    "Policy Evaluation",
    "Experimental Design",
    "Metrics Design",
    "Sports Analytics"
  ],
  "languages": ["Python"],
  "tools": [
    "NumPy",
    "Pandas",
    "Matplotlib",
    "Python Simulation Frameworks"
  ],
  "featured": 0,
  "featuredOrder": 3,
  "domain": "Sports",
  "year": 2024,
  "coverImage": "/projects/cfp-bias.png",
  "links": {
    "github": "https://github.com/PetersQuinn/CFPEvaluation",
    "paper": "https://github.com/PetersQuinn/CFPEvaluation/blob/main/Evaluation_of_the_CFP_Perception_Bug_Final.pdf",
    "demo": ""
  },
  "stats": [
    { "label": "Teams Simulated", "value": "134 FBS Teams" },
    { "label": "Season Length", "value": "12 Weeks" },
    { "label": "Monte Carlo Runs", "value": "100 per scenario" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Studied how perception-driven biases such as preseason rankings and media narratives influence College Football Playoff rankings.",
        "Focused on how incorrect initial beliefs propagate through rankings and affect playoff access, accuracy, and fairness."
      ]
    },
    {
      "title": "How",
      "body": [
        "Built simulation models representing both a small controlled league and the full 134-team FBS ecosystem.",
        "Initialized rankings using an intentionally inverted preseason poll to stress-test ranking inertia.",
        "Defined probabilistic game outcomes based on true team strength differentials.",
        "Designed a CFP-style points system and compared a standard committee approach against a harsher, outcome-driven variant.",
        "Tracked ranking evolution using custom metrics including AvgDiff, MaxDiff, MaxRise, and Top-25 specific accuracy measures."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Found that preseason bias meaningfully persists under conservative ranking systems, even with correct on-field outcomes.",
        "Demonstrated that harsher, performance-driven committees converge to true team strength faster but introduce greater early-season volatility.",
        "Quantified tradeoffs between ranking accuracy, stability, and fan-facing excitement, highlighting structural disadvantages for lower-perception teams."
      ]
    }
  ],
  "images": [
    "/projects/cfp-bias.png",
    "/projects/cfp-bias-1.png"
  ]
}
,
{
  "slug": "crypto-options-price-prediction",
  "title": "Crypto Options Price Prediction with Gradient Boosting Trees",
  "blurb": "Time-series-aware regression model that predicts BTC/ETH option trade prices from Binance data using engineered moneyness features and tuned GradientBoostingRegressor.",
  "highlights": [
    "Engineered log moneyness, signed moneyness, and ITM flags to capture option structure and nonlinearity.",
    "Used TimeSeriesSplit cross-validation and a custom composite objective (0.7·R² + 0.3·Normalized RMSE) for realistic out-of-sample tuning.",
    "Achieved test R² = 0.815 and normalized RMSE = 0.570, with permutation importance confirming core pricing drivers."
  ],
  "skills": [
    "Time Series Cross-Validation",
    "Feature Engineering",
    "Option Pricing Modeling",
    "Supervised Regression",
    "Hyperparameter Optimization",
    "Model Evaluation",
    "Permutation Importance"
  ],
  "languages": ["Python"],
  "tools": [
    "scikit-learn",
    "NumPy",
    "Pandas",
    "Matplotlib"
  ],
  "featured": 0,
  "featuredOrder": 3,
  "domain": "ML - Finance",
  "year": 2025,
  "coverImage": "/projects/crypto-options.png",
  "links": {
    "github": "",
    "paper": "",
    "demo": ""
  },
  "stats": [
    { "label": "Model", "value": "GradientBoostingRegressor" },
    { "label": "CV", "value": "TimeSeriesSplit (5-fold)" },
    { "label": "Test", "value": "R² = 0.815, NRMSE = 0.570" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built a regression model to predict cryptocurrency option trade prices using Binance BTC/ETH options transaction data.",
        "Focused on out-of-sample performance and interpretability by modeling how moneyness, time to maturity, and realized volatility relate to observed trade prices."
      ]
    },
    {
      "title": "How",
      "body": [
        "Preprocessed transaction data and encoded option type (call/put) and underlying asset (BTC/ETH), then engineered log moneyness, signed moneyness, and an in-the-money (ITM) flag.",
        "Benchmarked linear regression against tree ensembles, then selected GradientBoostingRegressor for nonlinear structure and stable generalization.",
        "Switched from shuffled KFold to TimeSeriesSplit to avoid overly optimistic validation and tuned hyperparameters with RandomizedSearchCV using a custom composite score (0.7·R² + 0.3·Normalized RMSE).",
        "Validated drivers with permutation importance and generated diagnostic plots including actual vs predicted pricing."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Achieved strong out-of-sample accuracy on the test set with R² = 0.815 and normalized RMSE = 0.570.",
        "Identified moneyness, underlying price, strike price, and realized volatility lags as the dominant drivers of predicted option prices.",
        "Produced a time-series-aware, reproducible modeling pipeline suitable for practical financial forecasting and evaluation."
      ]
    }
  ],
  "images": [
    "/projects/crypto-options.png",
    "/projects/crypto-options-1.png"
  ]
}
,
{
  "slug": "llm-to-sql-duckdb-poc",
  "title": "LLM-to-SQL Analytics POC with DuckDB",
  "blurb": "Node.js proof-of-concept that routes natural-language questions to safe, pre-defined SQL analytics functions over a DuckDB-backed dataset, then returns results in natural language using an OpenAI LLM.",
  "highlights": [
    "Built a lightweight analytics stack with Node.js + DuckDB over a CSV-backed view for fast local querying.",
    "Used an LLM as a JSON-only router to select a single allowed query function, reducing prompt-to-SQL risk.",
    "Returned human-readable answers by executing parameterized SQL and formatting results consistently."
  ],
  "skills": [
    "LLM Tool Routing",
    "Text-to-SQL Systems",
    "Prompt Engineering",
    "SQL Analytics",
    "API Integration",
    "Data Layer Design"
  ],
  "languages": ["JavaScript", "SQL"],
  "tools": [
    "Node.js",
    "DuckDB",
    "OpenAI API",
    "dotenv"
  ],
  "featured": 0,
  "featuredOrder": 6,
  "domain": "ML - Systems",
  "year": 2025,
  "coverImage": "/projects/llm-to-sql-poc.png",
  "links": {
    "github": "https://github.com/PetersQuinn/VoyaNodeJS",
    "paper": "",
    "demo": ""
  },
  "stats": [
    { "label": "Backend", "value": "DuckDB (CSV view)" },
    { "label": "Interface", "value": "Natural language → SQL analytics" },
    { "label": "Safety pattern", "value": "JSON router + whitelisted functions" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built a proof-of-concept assistant that answers natural-language analytics questions by translating intent into database queries and returning results in plain English.",
        "Instead of generating arbitrary SQL, the system routes user requests to a small set of approved analytics functions to keep execution predictable and safe."
      ]
    },
    {
      "title": "How",
      "body": [
        "Initialized an in-memory DuckDB instance and created a CSV-backed VIEW (fraud_data) for fast querying without standing up external infrastructure.",
        "Implemented a SQL layer with whitelisted functions (total records, fraud count, year summary) using parameterized queries to avoid injection and ensure consistent outputs.",
        "Used an OpenAI model as a strict JSON-only router that selects exactly one function to call and optionally extracts parameters such as a tax/form year.",
        "Wrapped everything in a simple CLI runner that initializes the data layer, calls the LLM router, executes the chosen SQL function, and formats a natural-language response."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Demonstrated reliable end-to-end behavior: natural language questions were mapped to the correct analytics function and answered with database-grounded results.",
        "Reduced text-to-SQL risk by constraining the LLM to structured routing decisions rather than free-form query generation.",
        "Delivered a small, modular architecture (data layer, SQL layer, LLM router, runner) that can be extended with additional safe analytics functions."
      ]
    }
  ],
  "images": [
    "/projects/llm-to-sql-poc.png",
    "/projects/llm-to-sql-poc-1.png"
  ]
}
,
{
  "slug": "water-rebate-cba-monte-carlo",
  "title": "Water Rebate Cost-Benefit Analysis with Monte Carlo Risk Modeling",
  "blurb": "Python cost-benefit and uncertainty analysis comparing high-efficiency toilet (HET) and smart irrigation controller (SIC) rebate programs, producing per-rebate and program-level NPV metrics with Monte Carlo sensitivity modeling.",
  "highlights": [
    "Built a deterministic CBA engine that computes per-rebate savings, PV savings, NPV, B/C ratio, payback, ROI, and gallons-per-dollar for HET vs SIC vs status quo.",
    "Scaled results to a one-year cohort program snapshot using eligible-household counts and annual adoption rates to estimate program PV savings, program NPV, and annual gallons saved.",
    "Implemented a Monte Carlo risk model with triangular parameter sampling plus water-price escalation and SIC climate variability to quantify distributional outcomes and P(NPV > 0)."
  ],
  "skills": [
    "Cost-Benefit Analysis",
    "Monte Carlo Simulation",
    "Risk Modeling",
    "Sensitivity Analysis",
    "NPV Modeling",
    "Scenario Modeling",
    "Impact Estimation",
    "Data Analysis"
  ],
  "languages": ["Python"],
  "tools": [
    "Pandas",
    "NumPy",
    "Matplotlib",
    "argparse"
  ],
  "featured": 0,
  "featuredOrder": 7,
  "domain": "Finance",
  "year": 2025,
  "coverImage": "/projects/water-rebate-cba.png",
  "links": {
    "github": "https://github.com/PetersQuinn/Group-Project",
    "paper": "https://github.com/PetersQuinn/Group-Project/blob/main/CEE351_Group_Project_Report.pdf",
    "demo": ""
  },
  "stats": [
    { "label": "Methods", "value": "Deterministic CBA + Monte Carlo" },
    { "label": "Distributions", "value": "Triangular (low/central/high)" },
    { "label": "Outputs", "value": "Per-rebate + program NPV distributions" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built a Python-based cost-benefit framework to compare two water conservation rebate programs: high-efficiency toilets (HET) and smart irrigation controllers (SIC), against a status quo baseline.",
        "Extended the deterministic analysis with a Monte Carlo uncertainty model to quantify risk, variability, and the probability of positive NPV under realistic parameter uncertainty."
      ]
    },
    {
      "title": "How",
      "body": [
        "Loaded a structured assumptions table and computed per-rebate economics including annual realized gallons saved (with rebound factors), annual dollar savings, present value savings via annuity factors, NPV, B/C ratio, payback, ROI, and gallons-per-dollar.",
        "Scaled per-rebate outputs into a one-year program snapshot using eligible-household counts and annual adoption rates to estimate total program cost, program PV savings, program NPV, and annual water savings.",
        "Ran Monte Carlo simulations by sampling each assumption from triangular distributions (low/central/high), adding water price escalation via a growing-annuity PV factor, and applying a SIC-only climate factor to reflect outdoor variability.",
        "Summarized outcome distributions with mean, percentiles, and P(NPV > 0), and generated histograms for both per-rebate and program-level NPVs."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Produced a reproducible decision-support pipeline that reports both point-estimate CBA metrics and full uncertainty distributions for HET and SIC program options.",
        "Enabled threshold-based policy interpretation using probability-of-positive-NPV and percentile bands to compare risk-adjusted performance across rebate alternatives.",
        "Delivered modular scripts (deterministic + Monte Carlo) that can be extended with additional technologies, alternative adoption assumptions, or expanded scenario ranges."
      ]
    }
  ],
  "images": [
    "/projects/water-rebate-cba.png"
  ]
}
,
{
  "slug": "sp500-index-replication-nn",
  "title": "S&P 500 Index Replication with Neural Networks",
  "blurb": "Neural network index-tracking pipeline that reconstructs S&P 500 returns using a small subset of constituents, optimizing both tracking error and portfolio efficiency under a reproducible, time-aware evaluation protocol.",
  "highlights": [
    "Converted 360 constituent price series into log returns, standardized inputs, and ranked stocks by Pearson correlation with the index to form a candidate universe.",
    "Trained a compact Keras MLP on top-correlated subsets and tuned subset size and hyperparameters via randomized search with a time-based validation split and early stopping.",
    "Replicated index behavior using ~30 stocks with RMSE ≈ 0.00799 on test returns, while explicitly trading off accuracy vs. portfolio efficiency (1 - n/360)."
  ],
  "skills": [
    "Index Replication",
    "Financial Time Series Modeling",
    "Feature Selection",
    "Neural Network Regression",
    "Time-Aware Validation",
    "Hyperparameter Search",
    "Reproducible ML"
  ],
  "languages": ["Python"],
  "tools": [
    "TensorFlow",
    "Keras",
    "scikit-learn",
    "NumPy",
    "Pandas",
    "Matplotlib"
  ],
  "featured": 0,
  "featuredOrder": 5,
  "domain": "ML - Finance",
  "year": 2025,
  "coverImage": "/projects/sp500-replication.png",
  "links": {
    "github": "",
    "paper": "",
    "demo": ""
  },
  "stats": [
    { "label": "Subset size", "value": "30 / 360 constituents" },
    { "label": "Objective", "value": "0.7·NRMSE + 0.3·Efficiency" },
    { "label": "Test RMSE", "value": "≈ 0.00799 (log-return scale)" }
  ],
  "sections": [
    {
      "title": "What",
      "body": [
        "Built a neural network system to replicate S&P 500 index dynamics using only a small subset of its 360 constituents.",
        "Optimized for real-world index tracking by combining normalized RMSE (tracking error proxy) with an explicit efficiency term that rewards smaller portfolios."
      ]
    },
    {
      "title": "How",
      "body": [
        "Transformed daily prices into log returns, aligned time indices, and standardized features with StandardScaler to stabilize training across diverse stocks.",
        "Ranked constituents by absolute Pearson correlation with the index to create an ordered universe and trained models on the top-k subsets.",
        "Implemented a time-aware validation split using the most recent portion of training data as validation, then tuned subset size and MLP hyperparameters with randomized sampling over learning rate, activation, batch size, and hidden units.",
        "Applied early stopping and fixed random seeds via tf.keras.utils.set_random_seed to ensure reproducible training and evaluation."
      ]
    },
    {
      "title": "Results",
      "body": [
        "Achieved strong out-of-sample tracking with RMSE ≈ 0.00799 on test log returns while using ~30 stocks.",
        "Produced a compact replication approach that explicitly balances tracking accuracy with portfolio simplicity via an efficiency-aware objective.",
        "Validated end-to-end reproducibility with fixed seeds and consistent grading on held-out test data, and visualized cumulative reconstructed index paths against true S&P 500."
      ]
    }
  ],
  "images": [
    "/projects/sp500-replication.png"
  ]
}


]
